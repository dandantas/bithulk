services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    env_file:
      - .env
    volumes:
      - .:/app
      - /app/node_modules
      - /app/.pnpm-store
    restart: unless-stopped

  # ollama:
  #   image: docker.io/ollama/ollama:latest
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - .:/code
  #     - ./ollama/ollama:/root/.ollama
  #     - ./ollama_entrypoint.sh:/ollama_entrypoint.sh
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   restart: always
  #   environment:
  #     - OLLAMA_KEEP_ALIVE=24h
  #     - OLLAMA_HOST=0.0.0.0
  #   entrypoint: ["/ollama_entrypoint.sh"]

  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: ollama-webui
  #   volumes:
  #     - ./ollama/ollama-webui:/app/backend/data
  #   depends_on:
  #     - ollama
  #   ports:
  #     - 8080:8080
  #   environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
  #     - OLLAMA_BASE_URLS=http://host.docker.internal:11434 #comma separated ollama hosts
  #     - ENV=dev
  #     - WEBUI_AUTH=False
  #     - WEBUI_NAME=valiantlynx AI
  #     - WEBUI_URL=http://localhost:8080
  #     - WEBUI_SECRET_KEY=t0p-s3cr3t
  #   extra_hosts:
  #     - host.docker.internal:host-gateway
  #   restart: unless-stopped
  #   networks:
  #     - ollama-docker

# networks:
#   ollama-docker:
#     external: false
